/**
 * Tests for Universal Query Router
 * 
 * Tests the unified interface that handles both:
 * 1. Tool selection (web_search, execute_code, file_search, artifacts)
 * 2. Model routing (4-tier system: simple â†’ expert)
 * 
 * 4-TIER MODEL MAPPING:
 * - simple:   Nova Micro   ($0.035/$0.14)  - Greetings, text-only simple responses (~1%)
 * - moderate: Haiku 4.5    ($1.00/$5.00)   - Most tasks, tool usage, standard code (~80%)
 * - complex:  Sonnet 4.5   ($3.00/$15.00)  - Debugging, detailed analysis (~15%)
 * - expert:   Opus 4.5     ($15.00/$75.00) - Deep analysis, architecture (~4%)
 */

import { 
  routeQuery, 
  Tool, 
  getModelForTier, 
  CLASSIFIER_MODEL,
  analyzeQuery,
  scoreQueryComplexity,
} from '../index';

describe('Universal Query Router', () => {
  describe('routeQuery', () => {
    it('should route simple greeting to simple tier with Nova Micro', async () => {
      const result = await routeQuery('Hello!', {
        provider: 'bedrock',
        preset: 'premium',
        availableTools: [Tool.WEB_SEARCH],
      });

      expect(result.tier).toBe('simple');
      expect(result.model).toContain('nova-micro');
      expect(result.tools).toEqual([]);
    });

    it('should route code query to appropriate tier with CODE_INTERPRETER', async () => {
      const result = await routeQuery('Write a Python function to sort a list', {
        provider: 'bedrock',
        preset: 'premium',
        availableTools: [Tool.CODE_INTERPRETER, Tool.WEB_SEARCH],
      });

      expect(result.tools).toContain('execute_code');
      expect(['simple', 'moderate', 'complex']).toContain(result.tier);
    });

    it('should route expert query to expert tier with Opus', async () => {
      const result = await routeQuery('Design a comprehensive microservices architecture for a banking system with event sourcing', {
        provider: 'bedrock',
        preset: 'premium',
        availableTools: [],
      });

      expect(['complex', 'expert']).toContain(result.tier);
      expect(result.model).toMatch(/opus|sonnet/i);
    });

    it('should use Sonnet for expert tier in costOptimized preset', async () => {
      const result = await routeQuery('Design a comprehensive microservices architecture', {
        provider: 'bedrock',
        preset: 'costOptimized',
        availableTools: [],
      });

      expect(['complex', 'expert']).toContain(result.tier);
      expect(result.model).toContain('sonnet');
    });

    it('should trigger LLM fallback for ambiguous queries', async () => {
      const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
        tools: ['web_search'],
        modelTier: 'simple',
        reasoning: 'Needs real-time data'
      }));

      const result = await routeQuery('What are booming stocks today?', {
        provider: 'bedrock',
        preset: 'premium',
        availableTools: [Tool.WEB_SEARCH, Tool.CODE_INTERPRETER],
        llmFallback: mockLlm,
      });

      expect(result.usedLlmFallback).toBe(true);
      expect(result.tools).toContain('web_search');
      expect(mockLlm).toHaveBeenCalled();
    });

    it('should NOT trigger LLM fallback for simple greetings', async () => {
      const mockLlm = jest.fn();

      const result = await routeQuery('Hi!', {
        provider: 'bedrock',
        preset: 'premium',
        availableTools: [Tool.WEB_SEARCH],
        llmFallback: mockLlm,
      });

      expect(result.usedLlmFallback).toBe(false);
      expect(mockLlm).not.toHaveBeenCalled();
    });
  });

  describe('getModelForTier', () => {
    it('should return correct models for premium preset (4-tier)', () => {
      expect(getModelForTier('simple', 'premium')).toContain('nova-micro');
      expect(getModelForTier('moderate', 'premium')).toContain('haiku');
      expect(getModelForTier('complex', 'premium')).toContain('sonnet');
      expect(getModelForTier('expert', 'premium')).toContain('opus');
    });

    it('should return correct models for costOptimized preset', () => {
      expect(getModelForTier('expert', 'costOptimized')).toContain('sonnet');
      expect(getModelForTier('complex', 'costOptimized')).toContain('sonnet');
      expect(getModelForTier('moderate', 'costOptimized')).toContain('haiku');
    });

    it('should default to costOptimized preset', () => {
      expect(getModelForTier('simple')).toContain('nova-micro');
    });
  });

  describe('CLASSIFIER_MODEL', () => {
    it('should be Nova Micro', () => {
      expect(CLASSIFIER_MODEL).toContain('nova-micro');
    });
  });

  describe('scoreQueryComplexity', () => {
    it('should score simple greeting as simple tier', () => {
      const result = scoreQueryComplexity('Hello');
      expect(result.tier).toBe('simple');
      expect(result.score).toBeLessThan(0.15);
    });

    it('should score code query appropriately', () => {
      const result = scoreQueryComplexity('Write a Python function');
      expect(['simple', 'moderate', 'complex']).toContain(result.tier);
      expect(result.categories).toContain('code');
    });

    it('should score complex architecture query as expert or complex', () => {
      const result = scoreQueryComplexity('Design a comprehensive microservices architecture with event sourcing and CQRS');
      expect(['complex', 'expert']).toContain(result.tier);
      expect(result.score).toBeGreaterThanOrEqual(0.6);
    });
  });

  describe('analyzeQuery', () => {
    it('should return both tool and model results', async () => {
      const result = await analyzeQuery({
        query: 'Calculate the sum of these numbers',
        availableTools: [Tool.CODE_INTERPRETER, Tool.WEB_SEARCH],
      });

      expect(result.tools).toBeDefined();
      expect(result.model).toBeDefined();
      expect(result.usedLlmFallback).toBe(false);
    });

    it('should work with empty tools', async () => {
      const result = await analyzeQuery({
        query: 'Hello',
        availableTools: [],
      });

      expect(result.tools.tools).toEqual([]);
      expect(result.model.tier).toBe('simple');
    });
  });
});

describe('Model Tier Mappings (4-tier system)', () => {
  /**
   * Expected model mappings for 4-tier routing:
   * - simple:   Nova Micro ($0.035/$0.14) - Greetings, text-only simple responses (~1%)
   * - moderate: Haiku 4.5  ($1/$5)        - Most tasks, tool usage, standard code (~80%)
   * - complex:  Sonnet 4.5 ($3/$15)       - Debugging, detailed analysis (~15%)
   * - expert:   Opus 4.5   ($15/$75)      - Deep analysis, architecture (~4%)
   */
  const expectedMappings = {
    premium: {
      simple: 'nova-micro',
      moderate: 'haiku',
      complex: 'sonnet',
      expert: 'opus',
    },
    costOptimized: {
      simple: 'nova-micro',
      moderate: 'haiku',
      complex: 'sonnet',
      expert: 'sonnet', // Capped at Sonnet
    },
    ultraCheap: {
      simple: 'nova-micro',
      moderate: 'haiku',
      complex: 'haiku',
      expert: 'haiku',   // Capped at Haiku
    },
  };

  Object.entries(expectedMappings).forEach(([preset, tiers]) => {
    describe(`${preset} preset`, () => {
      Object.entries(tiers).forEach(([tier, expectedModel]) => {
        it(`${tier} tier should use ${expectedModel}`, () => {
          const model = getModelForTier(tier as any, preset as any);
          expect(model.toLowerCase()).toContain(expectedModel);
        });
      });
    });
  });
});

describe('Selection Response Handling', () => {
  /**
   * Tests for short selection responses like "1", "2", "a", "b"
   * These should trigger LLM fallback when conversation history is present
   * to understand the context of the selection.
   */

  describe('Numeric selection responses', () => {
    it('should trigger LLM fallback for "1" with conversation history', async () => {
      const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
        tools: ['artifacts'],
        modelTier: 'complex',
        reasoning: 'User selected option 1 (Interactive React component)'
      }));

      const conversationHistory = [
        { role: 'user' as const, content: 'Visualize this data' },
        { role: 'assistant' as const, content: 'I can create this in different ways. Which would you prefer?\n1. Interactive React component\n2. Python chart (matplotlib)' }
      ];

      const result = await analyzeQuery({
        query: '1',
        availableTools: [Tool.ARTIFACTS, Tool.CODE_INTERPRETER],
        conversationHistory,
        llmFallback: mockLlm,
      });

      expect(mockLlm).toHaveBeenCalled();
      expect(result.usedLlmFallback).toBe(true);
    });

    it('should trigger LLM fallback for "2" with conversation history', async () => {
      const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
        tools: ['execute_code'],
        modelTier: 'simple',
        reasoning: 'User selected option 2 (Python chart)'
      }));

      const conversationHistory = [
        { role: 'user' as const, content: 'Show me a chart' },
        { role: 'assistant' as const, content: 'Which type?\n1. React component\n2. Python matplotlib' }
      ];

      const result = await analyzeQuery({
        query: '2',
        availableTools: [Tool.ARTIFACTS, Tool.CODE_INTERPRETER],
        conversationHistory,
        llmFallback: mockLlm,
      });

      expect(mockLlm).toHaveBeenCalled();
      expect(result.usedLlmFallback).toBe(true);
    });

    it('should trigger LLM fallback for single digit "3" with conversation history', async () => {
      const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
        tools: ['web_search'],
        modelTier: 'simple',
        reasoning: 'User selected option 3'
      }));

      const conversationHistory = [
        { role: 'user' as const, content: 'Help me' },
        { role: 'assistant' as const, content: '1. Search web\n2. Run code\n3. Search files' }
      ];

      const result = await analyzeQuery({
        query: '3',
        availableTools: [Tool.WEB_SEARCH, Tool.CODE_INTERPRETER, Tool.FILE_SEARCH],
        conversationHistory,
        llmFallback: mockLlm,
      });

      expect(mockLlm).toHaveBeenCalled();
      expect(result.usedLlmFallback).toBe(true);
    });
  });

  describe('Letter selection responses', () => {
    it('should trigger LLM fallback for "a" with conversation history', async () => {
      const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
        tools: ['artifacts'],
        modelTier: 'complex',
        reasoning: 'User selected option A'
      }));

      const conversationHistory = [
        { role: 'user' as const, content: 'Create a visualization' },
        { role: 'assistant' as const, content: 'Options:\nA. React dashboard\nB. Python plot' }
      ];

      const result = await analyzeQuery({
        query: 'a',
        availableTools: [Tool.ARTIFACTS, Tool.CODE_INTERPRETER],
        conversationHistory,
        llmFallback: mockLlm,
      });

      expect(mockLlm).toHaveBeenCalled();
      expect(result.usedLlmFallback).toBe(true);
    });

    it('should trigger LLM fallback for "B" (uppercase) with conversation history', async () => {
      const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
        tools: ['execute_code'],
        modelTier: 'simple',
        reasoning: 'User selected option B'
      }));

      const conversationHistory = [
        { role: 'user' as const, content: 'Make a chart' },
        { role: 'assistant' as const, content: 'A. Interactive\nB. Static Python' }
      ];

      const result = await analyzeQuery({
        query: 'B',
        availableTools: [Tool.ARTIFACTS, Tool.CODE_INTERPRETER],
        conversationHistory,
        llmFallback: mockLlm,
      });

      expect(mockLlm).toHaveBeenCalled();
      expect(result.usedLlmFallback).toBe(true);
    });
  });

  describe('Short queries with conversation history', () => {
    it('should trigger LLM fallback for short follow-up "yes" with conversation history', async () => {
      // Note: "yes" is in the greeting list but should still trigger LLM if there's history
      // Actually "yes" is a simple greeting - let's test "ok" isn't but short query is
      const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
        tools: ['artifacts'],
        modelTier: 'complex',
        reasoning: 'User confirmed with short response'
      }));

      const conversationHistory = [
        { role: 'user' as const, content: 'Build a dashboard' },
        { role: 'assistant' as const, content: 'Shall I create a React dashboard?' }
      ];

      // "go" is short and not in greeting list
      const result = await analyzeQuery({
        query: 'go',
        availableTools: [Tool.ARTIFACTS],
        conversationHistory,
        llmFallback: mockLlm,
      });

      expect(mockLlm).toHaveBeenCalled();
      expect(result.usedLlmFallback).toBe(true);
    });

    it('should trigger LLM fallback for "do it" with conversation history', async () => {
      const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
        tools: ['execute_code'],
        modelTier: 'simple',
        reasoning: 'User confirmed action'
      }));

      const conversationHistory = [
        { role: 'user' as const, content: 'Analyze this CSV' },
        { role: 'assistant' as const, content: 'I can process this with Python. Proceed?' }
      ];

      const result = await analyzeQuery({
        query: 'do it',
        availableTools: [Tool.CODE_INTERPRETER],
        conversationHistory,
        llmFallback: mockLlm,
      });

      expect(mockLlm).toHaveBeenCalled();
      expect(result.usedLlmFallback).toBe(true);
    });
  });

  describe('Edge cases', () => {
    it('should NOT trigger LLM for selection without conversation history', async () => {
      const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
        tools: [],
        modelTier: 'simple',
        reasoning: 'No context'
      }));

      // No conversation history - "1" alone has no context
      const result = await analyzeQuery({
        query: '1',
        availableTools: [Tool.ARTIFACTS, Tool.CODE_INTERPRETER],
        llmFallback: mockLlm,
      });

      // Without history, short query shouldn't trigger LLM (no context to understand)
      // Actually with our new logic, isSelectionResponse will trigger it anyway
      // Let's verify the behavior
      expect(mockLlm).toHaveBeenCalled(); // Selection pattern triggers LLM
    });

    it('should NOT trigger LLM for simple greetings even with conversation history', async () => {
      const mockLlm = jest.fn();

      const conversationHistory = [
        { role: 'user' as const, content: 'Hello' },
        { role: 'assistant' as const, content: 'Hi there!' }
      ];

      const result = await analyzeQuery({
        query: 'thanks',
        availableTools: [Tool.WEB_SEARCH],
        conversationHistory,
        llmFallback: mockLlm,
      });

      // "thanks" is in the simple greeting list, should not trigger LLM
      expect(mockLlm).not.toHaveBeenCalled();
      expect(result.usedLlmFallback).toBe(false);
    });

    it('should NOT trigger LLM for "hello" even with conversation history', async () => {
      const mockLlm = jest.fn();

      const conversationHistory = [
        { role: 'user' as const, content: 'Previous message' },
        { role: 'assistant' as const, content: 'Previous response' }
      ];

      const result = await analyzeQuery({
        query: 'hello',
        availableTools: [Tool.WEB_SEARCH],
        conversationHistory,
        llmFallback: mockLlm,
      });

      // "hello" is a simple greeting, should not trigger LLM
      expect(mockLlm).not.toHaveBeenCalled();
      expect(result.usedLlmFallback).toBe(false);
    });

    it('should handle selection with period "1."', async () => {
      const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
        tools: ['artifacts'],
        modelTier: 'complex',
        reasoning: 'User selected option 1'
      }));

      const conversationHistory = [
        { role: 'user' as const, content: 'Create something' },
        { role: 'assistant' as const, content: '1. React\n2. Python' }
      ];

      const result = await analyzeQuery({
        query: '1.',
        availableTools: [Tool.ARTIFACTS, Tool.CODE_INTERPRETER],
        conversationHistory,
        llmFallback: mockLlm,
      });

      expect(mockLlm).toHaveBeenCalled();
      expect(result.usedLlmFallback).toBe(true);
    });
  });
});

describe('Conversation History Context', () => {
  /**
   * Tests that conversation history is properly passed to LLM
   * for understanding follow-up queries
   */

  it('should pass conversation history to LLM fallback', async () => {
    let capturedPrompt = '';
    const mockLlm = jest.fn().mockImplementation((prompt: string) => {
      capturedPrompt = prompt;
      return Promise.resolve(JSON.stringify({
        tools: ['web_search'],
        modelTier: 'simple',
        reasoning: 'Follow-up query needs context'
      }));
    });

    const conversationHistory = [
      { role: 'user' as const, content: 'What stocks are trending?' },
      { role: 'assistant' as const, content: 'Here are trending stocks: AAPL, NVDA, TSLA' }
    ];

    await analyzeQuery({
      query: 'more details',
      availableTools: [Tool.WEB_SEARCH],
      conversationHistory,
      llmFallback: mockLlm,
    });

    // Verify conversation history was included in prompt
    expect(capturedPrompt).toContain('stocks');
    expect(capturedPrompt.toLowerCase()).toContain('conversation');
  });

  it('should understand follow-up visualization request from history', async () => {
    const mockLlm = jest.fn().mockResolvedValue(JSON.stringify({
      tools: ['artifacts'],
      modelTier: 'complex',
      reasoning: 'User wants to visualize the data from previous message'
    }));

    const conversationHistory = [
      { role: 'user' as const, content: 'Give me sales data' },
      { role: 'assistant' as const, content: 'Here is the sales data:\nQ1: $100k\nQ2: $150k\nQ3: $120k\nQ4: $200k' }
    ];

    const result = await analyzeQuery({
      query: 'chart this',
      availableTools: [Tool.ARTIFACTS, Tool.CODE_INTERPRETER],
      conversationHistory,
      llmFallback: mockLlm,
    });

    expect(mockLlm).toHaveBeenCalled();
    expect(result.usedLlmFallback).toBe(true);
  });
});
